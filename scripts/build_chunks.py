"""
Build chunks JSONL file from cleaned `.mmd` books.

Pipeline:
1. Assume `books/mmd_clean/` has been generated by `scripts/preprocess/clean_mmd.py`.
2. Use `structural_chunker` to create structural chunks.
3. Exclude references, exercises, appendix, bibliography from QA-oriented output.
4. Attach `key_terms`, `potential_questions`, and `subject` (os/dbms/cn).
5. Write to `data/chunks.jsonl`.
"""

from __future__ import annotations

import json
from pathlib import Path

from scripts.chunking.metadata_extractor import extract_key_terms, extract_potential_questions
from scripts.chunking.structural_chunker import chunk_books_in_dir, is_qa_excluded

ROOT = Path(__file__).resolve().parents[1]
BOOKS_DIR = ROOT / "books" / "mmd_clean"
OUT_PATH = ROOT / "data" / "chunks.jsonl"

# Map book_id (substring match) to subject for QA filtering and tagging.
SUBJECT_BY_BOOK: list[tuple[str, str]] = [
    ("Computer Networking", "cn"),
    ("Computer Networks", "cn"),
    ("Database Management", "dbms"),
    ("Database System", "dbms"),
    ("Fundamentals of Database", "dbms"),
    ("Modern Operating", "os"),
    ("Operating System Concepts", "os"),
    ("Operating Systems Three", "os"),
]


def infer_subject(book_id: str) -> str:
    """Infer subject (os, dbms, cn) from book_id."""
    bid_lower = book_id.lower()
    for key, subject in SUBJECT_BY_BOOK:
        if key.lower() in bid_lower:
            return subject
    return "os"


def main() -> None:
    if not BOOKS_DIR.exists():
        raise SystemExit(
            f"Expected cleaned .mmd directory at {BOOKS_DIR}. "
            "Run scripts/preprocess/clean_mmd.py first."
        )

    print(f"Chunking books from {BOOKS_DIR} ...")
    chunks = chunk_books_in_dir(BOOKS_DIR)
    print(f"Got {len(chunks)} structural chunks.")

    # Exclude references, exercises, appendix, bibliography for QA
    included = [ch for ch in chunks if not is_qa_excluded(ch)]
    excluded_count = len(chunks) - len(included)
    if excluded_count:
        print(f"Excluded {excluded_count} chunks (references, exercises, appendix, etc.).")
    print(f"Writing {len(included)} chunks.")

    OUT_PATH.parent.mkdir(parents=True, exist_ok=True)
    written = 0
    with OUT_PATH.open("w", encoding="utf-8", newline="\n") as f:
        for ch in included:
            key_terms = extract_key_terms(ch.text)
            potential_questions = extract_potential_questions(
                ch.header_path,
                ch.chunk_type,
                ch.text,
            )
            subject = infer_subject(ch.book_id)
            obj = {
                "id": ch.id,
                "book_id": ch.book_id,
                "header_path": ch.header_path,
                "chunk_type": ch.chunk_type,
                "key_terms": key_terms,
                "text": ch.text,
                "potential_questions": potential_questions,
                "subject": subject,
            }
            f.write(json.dumps(obj, ensure_ascii=False) + "\n")
            written += 1

    print(f"Wrote {written} chunks to {OUT_PATH}")


if __name__ == "__main__":
    main()
