"""
One-shot script to build a chunks JSONL file from the cleaned `.mmd` books.

Pipeline:
1. Assume `books/mmd_clean/` has been generated by `eval/preprocess/clean_mmd.py`.
2. Use `structural_chunker` to create structural chunks.
3. Use `metadata_extractor` to attach `key_terms`.
4. Write all chunks to `eval/dataset/chunks.jsonl`.
"""

from __future__ import annotations

import json
from pathlib import Path

from chunking.structural_chunker import chunk_books_in_dir
from chunking.metadata_extractor import extract_key_terms


ROOT = Path(__file__).resolve().parents[1]
BOOKS_DIR = ROOT / "books" / "mmd_clean"
OUT_DIR = ROOT / "eval" / "dataset"
OUT_PATH = OUT_DIR / "chunks.jsonl"


def main() -> None:
    if not BOOKS_DIR.exists():
        raise SystemExit(
            f"Expected cleaned .mmd directory at {BOOKS_DIR}. "
            "Run eval/preprocess/clean_mmd.py first."
        )

    print(f"Chunking books from {BOOKS_DIR} ...")
    chunks = chunk_books_in_dir(BOOKS_DIR)
    print(f"Got {len(chunks)} structural chunks.")

    OUT_DIR.mkdir(parents=True, exist_ok=True)
    written = 0
    with OUT_PATH.open("w", encoding="utf-8", newline="\n") as f:
        for ch in chunks:
            key_terms = extract_key_terms(ch.text)
            obj = {
                "id": ch.id,
                "book_id": ch.book_id,
                "header_path": ch.header_path,
                "chunk_type": ch.chunk_type,
                "key_terms": key_terms,
                "text": ch.text,
            }
            f.write(json.dumps(obj, ensure_ascii=False) + "\n")
            written += 1

    print(f"Wrote {written} chunks to {OUT_PATH}")


if __name__ == "__main__":
    main()

